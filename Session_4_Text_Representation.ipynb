{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZ3Rb6JRfF3"
      },
      "source": [
        "# Text Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wPRdi49SLbkF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import os\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiqJ51lOLbkH",
        "outputId": "e77e0fa8-4fad-4457-b8d1-f7562bdb9150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-13 20:58:54--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_All_Beauty.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10329961 (9.9M) [application/octet-stream]\n",
            "Saving to: ‘meta_All_Beauty.json.gz’\n",
            "\n",
            "meta_All_Beauty.jso 100%[===================>]   9.85M  6.93MB/s    in 1.4s    \n",
            "\n",
            "2022-03-13 20:58:56 (6.93 MB/s) - ‘meta_All_Beauty.json.gz’ saved [10329961/10329961]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_All_Beauty.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFcrUOWCL_Ld",
        "outputId": "8c01ee43-f657-4161-a950-2c2e90f42350"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=c3f3d0aae2e0993f3f57d0ac995bf60e8f6da2ca702cc7174be2cac93217d373\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIYb2Pxi6_Ie"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (nor in training or test sets). Apply preprocessing (stemming and stopwords removal) to clean up the text from the \"title\". Report the vocabulary size before and after the preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CKbUQSlc65kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9536cc9d-7299-41bb-f6f1-88b2a8d9f23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32892\n",
            "32488\n",
            "84\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import import_ipynb\n",
        "\n",
        "\n",
        "# Load TRAIN and TEST sets \n",
        "from Session_1 import training_data, test_data\n",
        "\n",
        "# Load the METADATA (ITEMS)\n",
        "def parse(path):\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for l in g:\n",
        "        yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('meta_All_Beauty.json.gz')\n",
        "print(len(df))\n",
        "\n",
        "# Discard duplicates\n",
        "df = df.drop_duplicates(subset=['asin']).reset_index(drop=True)\n",
        "print(len(df))\n",
        "\n",
        "# Discard items that weren't rated by our subset of users\n",
        "item_in_training = df['asin'].isin(training_data.append(test_data)['asin'])\n",
        "df = df[item_in_training].reset_index(drop=True)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rd0RbH-k9y1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791e251c-c14f-4e9f-f7bb-333c6a4f63ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "443\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string \n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "def clean_non_alpha(text):\n",
        "    return re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "#storing the puntuation free text\n",
        "df['title_remove']= df['title'].apply(clean_non_alpha)\n",
        "df['clean_title']= df['title_remove'].apply(lambda x: x.lower())\n",
        "df['clean_title']= df['clean_title'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# the vocabulary size before the preprocessing.\n",
        "vacabulary = set()\n",
        "\n",
        "for word in df.clean_title.values:\n",
        "    for voc in word:\n",
        "        vacabulary.add(voc)\n",
        "\n",
        "print(len(vacabulary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz8K6K9rLbkL",
        "outputId": "1f1482f9-d7e3-4761-baf9-d11adf78cb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       [aqua, velva, shave, classic, ice, blue, ounce]\n",
            "1      [citre, shine, moisture, burst, shampoo, fl, oz]\n",
            "2                             [nars, blush, taj, mahal]\n",
            "3     [avalon, organics, wrinkle, therapy, coq, clea...\n",
            "4                         [bar, anise, lavender, ounce]\n",
            "                            ...                        \n",
            "79          [ultimate, body, lotion, michael, kors, oz]\n",
            "80        [dolce, amp, gabbana, compact, parfum, ounce]\n",
            "81    [colgate, kids, maximum, cavity, protection, p...\n",
            "82    [bali, secrets, natural, deodorant, organic, a...\n",
            "83                  [essie, gel, couture, nail, polish]\n",
            "Name: clean_title, Length: 84, dtype: object\n",
            "389\n",
            "0        [aqua, velva, shave, classic, ice, blue, ounc]\n",
            "1        [citr, shine, moistur, burst, shampoo, fl, oz]\n",
            "2                              [nar, blush, taj, mahal]\n",
            "3     [avalon, organ, wrinkl, therapi, coq, cleans, ...\n",
            "4                             [bar, anis, lavend, ounc]\n",
            "                            ...                        \n",
            "79              [ultim, bodi, lotion, michael, kor, oz]\n",
            "80          [dolc, amp, gabbana, compact, parfum, ounc]\n",
            "81    [colgat, kid, maximum, caviti, protect, pump, ...\n",
            "82    [bali, secret, natur, deodor, organ, amp, vega...\n",
            "83                    [essi, gel, coutur, nail, polish]\n",
            "Name: clean_title, Length: 84, dtype: object\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words()\n",
        "\n",
        "# stopwords removal\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stop_words]\n",
        "    return output\n",
        "\n",
        "df['clean_title'] = df['clean_title'].apply(remove_stopwords)\n",
        "print(df.clean_title)\n",
        "\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text\n",
        "df['clean_title'] = df['clean_title'].apply(lambda x: stemming(x))\n",
        "\n",
        "# the vocabulary size after the preprocessing.\n",
        "vacabulary = set()\n",
        "for word in df.clean_title.values:\n",
        "    for voc in word:\n",
        "        vacabulary.add(voc)\n",
        "print(len(vacabulary))\n",
        "\n",
        "print(df.clean_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blr1jgoHLbFU"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "Representation in vector spaces.\n",
        "\n",
        "## 2.1\n",
        "\n",
        "Represent all the products from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
        "\n",
        "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vDndolvDLznV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "tf_idf_matrix = tfidf_vectorizer.fit_transform(df.title_remove.values)\n",
        "tf_idf_array = tf_idf_matrix.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dei-zFzSLbkN",
        "outputId": "3f873a41-79b8-407b-f4a6-1effbee58103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (84, 432)\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF matrix shape:\", tf_idf_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gnTVM0EV_2d"
      },
      "source": [
        "## 2.2\n",
        "\n",
        "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'. Take a look at their features to see whether results make sense with their characteristics. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5CsNkzGLbkO",
        "outputId": "448048d8-f43f-4162-f8a0-6078730917c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'B000FI4S1E'and 'B000LIBUBY': 0.033\n",
            "Similarity between 'B000FI4S1E'and 'B000W0C07Y': 0.023\n",
            "Similarity between 'B000LIBUBY'and 'B000W0C07Y': 0.318\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "item_weight_matrix = pd.DataFrame(index=df.asin.values, data=tf_idf_array)\n",
        "\n",
        "similarity_1e_by = cosine_similarity([item_weight_matrix.loc['B000FI4S1E'].values], [item_weight_matrix.loc['B000LIBUBY'].values])\n",
        "print(\"Similarity between 'B000FI4S1E'and 'B000LIBUBY':\", round(similarity_1e_by[0, 0], 3))\n",
        "\n",
        "similarity_1e_7y = cosine_similarity([item_weight_matrix.loc['B000FI4S1E'].values], [item_weight_matrix.loc['B000W0C07Y'].values])\n",
        "print(\"Similarity between 'B000FI4S1E'and 'B000W0C07Y':\", round(similarity_1e_7y[0, 0], 3))\n",
        "\n",
        "similarity_by_7y = cosine_similarity([item_weight_matrix.loc['B000LIBUBY'].values], [item_weight_matrix.loc['B000W0C07Y'].values])\n",
        "print(\"Similarity between 'B000LIBUBY'and 'B000W0C07Y':\", round(similarity_by_7y[0, 0], 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06A6IWEYMbKp",
        "outputId": "0ed729b2-98d0-4fef-ed0d-4f1d13a0a113"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8jRhWhZQWe"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Representation in vector spaces with contextual Word Embeddings.\n",
        "\n",
        "## 3.1.\n",
        "\n",
        "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the 'title'. What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
        "\n",
        "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hHIjJ-LbTB3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2fc7774-ef7f-415d-da1b-da9658011e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# LOAD TRANSFORMER\n",
        "\"\"\"\n",
        "If you plan on using a pretrained model, it’s important to use the associated \n",
        "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
        "for the pretraining corpus, and it will use the same correspondence\n",
        "token to index (that we usually call a vocab) as during pretraining.\n",
        "\"\"\"\n",
        "\n",
        "# % pip install transformers\n",
        "import torch\n",
        "import transformers\n",
        "assert transformers.__version__ > '4.0.0'\n",
        "\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "\n",
        "# set-up environment\n",
        "# DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "modelname = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
        "model = BertModel.from_pretrained(modelname)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the vocabulary size\n",
        "# <YOUR CODE HERE>\n",
        "configuration = model.config\n",
        "print(\"Vocabulary size of the model:\", configuration.vocab_size)\n",
        "print(\"Input dimension:\", configuration.hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Vdzz_UPVik",
        "outputId": "a50bbbb8-ebf7-45b9-f041-153a69725e7d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size of the model: 30522\n",
            "Input dimension: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "q_Symyv5U07x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1470b4ac-ab65-454b-9d98-61748f8203ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 28319,  2310,  ...,     0,     0,     0],\n",
            "        [  101, 25022,  7913,  ...,     0,     0,     0],\n",
            "        [  101,  6583,  2869,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  8902,  5867,  ...,     0,     0,     0],\n",
            "        [  101, 20222,  7800,  ...,  1033,   102,     0],\n",
            "        [  101,  9686, 11741,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "last_hidden_states: torch.Size([84, 52, 768])\n"
          ]
        }
      ],
      "source": [
        "# REPRESENT PRODUCTS IN A VECTOR SPACE\n",
        "def batch_encoding(sentences):\n",
        "    # Since we're using padding, we need to provide the attention masks to our\n",
        "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
        "\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "    print(inputs) # Look at the padding and attention_mask \n",
        "    # attention mask is a binary tensor indicating the position of the padded indices \n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    return inputs, last_hidden_states\n",
        "\n",
        "encoded_inputs, title_last_hidden_states = batch_encoding([text for text in df.title.values])\n",
        "\n",
        "\"\"\"\n",
        "Note that the control token [CLS] has been added \n",
        "at the beginning of each sentence, and [SEP] at the end. \n",
        "\"\"\"\n",
        "\n",
        "# Now, let's mask out the padding tokens and compute the embedding vector of each product\n",
        "print(\"last_hidden_states:\", title_last_hidden_states.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding vector of each product\n",
        "embedding_vectors = torch.mean(title_last_hidden_states, dim=1)\n",
        "embedding_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqS7-d7T-di",
        "outputId": "7c6e0228-821e-4720-d914-6defbcf8e18c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([84, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequence = tokenizer.decode(encoded_inputs[\"input_ids\"][0])\n",
        "encoded_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zWZyzZgpMf1N",
        "outputId": "9582029d-9a45-4b01-c5e8-7fae342ce4cb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] aqua velva after shave, classic ice blue, 7 ounce [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRBr2HP0Zdt"
      },
      "source": [
        "## 3.2.\n",
        "\n",
        "Compute and the cosine similarity between products with asin 'B000FI4S1E', 'B000LIBUBY' and 'B000W0C07Y'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "OaHxSLHqItNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1d267d-b6ad-4d13-fb3f-34dc9c27de90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'B000FI4S1E'and 'B000LIBUBY': 0.836\n",
            "Similarity between 'B000FI4S1E'and 'B000W0C07Y': 0.759\n",
            "Similarity between 'B000LIBUBY'and 'B000W0C07Y': 0.754\n"
          ]
        }
      ],
      "source": [
        "item_ids_matrix = pd.DataFrame(index=df.asin.values, data=embedding_vectors)\n",
        "\n",
        "similarity_1e_by_ids = cosine_similarity([item_ids_matrix.loc['B000FI4S1E'].values], [item_ids_matrix.loc['B000LIBUBY'].values])\n",
        "print(\"Similarity between 'B000FI4S1E'and 'B000LIBUBY':\", round(similarity_1e_by_ids[0, 0], 3))\n",
        "\n",
        "similarity_1e_7y_ids = cosine_similarity([item_ids_matrix.loc['B000FI4S1E'].values], [item_ids_matrix.loc['B000W0C07Y'].values])\n",
        "print(\"Similarity between 'B000FI4S1E'and 'B000W0C07Y':\", round(similarity_1e_7y_ids[0, 0], 3))\n",
        "\n",
        "similarity_by_7y_ids = cosine_similarity([item_ids_matrix.loc['B000LIBUBY'].values], [item_ids_matrix.loc['B000W0C07Y'].values])\n",
        "print(\"Similarity between 'B000LIBUBY'and 'B000W0C07Y':\", round(similarity_by_7y_ids[0, 0], 3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Session_4_Text_Representation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}