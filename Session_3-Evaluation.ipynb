{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) models implemented last week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Load the test set and the predictions made with both Collaborative Filtering models in the previous session. \n",
    "2. Detect those users which are in the training set but not in the test set. Remove their predictions before evaluating the systems.\n",
    "3. Report the Root Mean Square Error (RMSE) for both CF models defined in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73a72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 users in the training set that are not in the test set.\n",
      "Evaluating the systems with 52988 predictions for users in the test split.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "\n",
    "# TEST\n",
    "from Session_1 import test_data\n",
    "\n",
    "# PREDICTIONS\n",
    "from Session_2 import predictions_neigbor, predictions_factor\n",
    "\n",
    "pred_nb_list = predictions_neigbor\n",
    "pred_lf_list = predictions_factor\n",
    "# Detect users from training set that are not in test\n",
    "nb_users = set([pred.uid for pred in pred_nb_list])\n",
    "lf_users = set([pred.uid for pred in pred_lf_list])\n",
    "nb_users_in_pred_but_not_in_test = list(nb_users.difference(set(test_data['reviewerID'])))\n",
    "lf_users_in_pred_but_not_in_test = list(lf_users.difference(set(test_data['reviewerID'])))\n",
    "assert nb_users_in_pred_but_not_in_test == lf_users_in_pred_but_not_in_test\n",
    "print(f\"There are {len(lf_users_in_pred_but_not_in_test)} users in the training set that are not in the test set.\")\n",
    "\n",
    "# Remove these users' predictions for evaluation\n",
    "pred_nb_list_removed = [pred for pred in pred_nb_list if pred.uid not in nb_users_in_pred_but_not_in_test]\n",
    "pred_lf_list_removed = [pred for pred in pred_lf_list if pred.uid not in lf_users_in_pred_but_not_in_test]\n",
    "assert len(pred_nb_list_removed) == len(pred_lf_list_removed)\n",
    "print(f\"Evaluating the systems with {len(pred_nb_list_removed)} predictions for users in the test split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6856\n",
      "RMSE for Neighborhood based Collaborative Filtering: 0.686\n",
      "RMSE: 0.5486\n",
      "RMSE for Latent Factor based Collaborative Filtering: 0.549\n"
     ]
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "print(\"RMSE for Neighborhood based Collaborative Filtering: {:.3f}\".format(accuracy.rmse(pred_nb_list_removed)))\n",
    "print(\"RMSE for Latent Factor based Collaborative Filtering: {:.3f}\".format(accuracy.rmse(pred_lf_list_removed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Define a general method to get the top-k recommendations for each user. Print the top-k with k={5, 10} recommendations for the user with ID 'ARARUVZ8RUF5T' and its estimated ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c95e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhood based Collaborative Filtering:\n",
      "TOP-5 predictions for user ARARUVZ8RUF5T: [('B000WR2HB6', 5), ('B000FOI48G', 4.675), ('B000VV1YOY', 4.666666666666667), ('B001ET7FZE', 4.6), ('B000PKKAGO', 4.5)]\n",
      "TOP-10 predictions for user ARARUVZ8RUF5T: [('B000WR2HB6', 5), ('B000FOI48G', 4.675), ('B000VV1YOY', 4.666666666666667), ('B001ET7FZE', 4.6), ('B000PKKAGO', 4.5), ('B00EF1QRMU', 4.470205150915517), ('B016V8YWBC', 4.458333333333333), ('B00W259T7G', 4.42), ('B00CZH3K1C', 4.333333333333334), ('B000GLRREU', 4.233333333333333)]\n",
      "Latent Factor based Collaborative Filtering:\n",
      "TOP-5 predictions for user ARARUVZ8RUF5T: [('B006IB5T4W', 5), ('B001F51RAG', 5), ('B00NT0AR7E', 5), ('B006WYJM8Y', 5), ('B00155Z6V2', 5)]\n",
      "TOP-10 predictions for user ARARUVZ8RUF5T: [('B006IB5T4W', 5), ('B001F51RAG', 5), ('B00NT0AR7E', 5), ('B006WYJM8Y', 5), ('B00155Z6V2', 5), ('B00021DJ32', 4.993923352784167), ('B001QY8QXM', 4.948791090341968), ('B000X7ST9Y', 4.840934395462433), ('B00126LYJM', 4.839149265457805), ('B002RZZXYE', 4.834041803397214)]\n"
     ]
    }
   ],
   "source": [
    "def transfer_to_user_item_rating(pred_list):\n",
    "    # First map the predictions to each user.\n",
    "    user_item_rating = defaultdict(list)\n",
    "    for uid, iid, _, est, _ in pred_list:\n",
    "        user_item_rating[uid].append((iid, est))\n",
    "    # {uid: (iid, est)}\n",
    "    return user_item_rating\n",
    "\n",
    "def top_k_recommendations(n, user_item_rating):\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, user_ratings in user_item_rating.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        # {uid: (iid, est)}\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "user_item_rating_nb = transfer_to_user_item_rating(pred_nb_list_removed)\n",
    "user_item_rating_lf = transfer_to_user_item_rating(pred_lf_list_removed)\n",
    "\n",
    "print(\"Neighborhood based Collaborative Filtering:\")\n",
    "top_5_nb = top_k_recommendations(5, user_item_rating_nb)\n",
    "print(f\"TOP-5 predictions for user ARARUVZ8RUF5T:\", top_5_nb[\"ARARUVZ8RUF5T\"]) \n",
    "top_10_nb = top_k_recommendations(10, user_item_rating_nb)\n",
    "print(f\"TOP-10 predictions for user ARARUVZ8RUF5T:\", top_10_nb[\"ARARUVZ8RUF5T\"]) \n",
    "top_20_nb = top_k_recommendations(20, user_item_rating_nb)\n",
    "\n",
    "print(\"Latent Factor based Collaborative Filtering:\")   \n",
    "top_5_lf = top_k_recommendations(5, user_item_rating_lf)\n",
    "print(f\"TOP-5 predictions for user ARARUVZ8RUF5T:\", top_5_lf[\"ARARUVZ8RUF5T\"]) \n",
    "top_10_lf = top_k_recommendations(10, user_item_rating_lf)\n",
    "print(f\"TOP-10 predictions for user ARARUVZ8RUF5T:\", top_10_lf[\"ARARUVZ8RUF5T\"]) \n",
    "top_20_lf = top_k_recommendations(20, user_item_rating_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Excercise 3\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with k={5, 10, 20} averaged across users for both CF systems. When computing precision, we consider as relevant items those with an observed rating >= 4.0 (i.e., those items from the test set with a rating >= 4.0). Reflect on the differences obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb26be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "user_item_test = test_data.pivot('reviewerID', 'asin', 'overall')\n",
    "user_item_test = user_item_test.fillna(0)\n",
    "\n",
    "# compute P@k for one user\n",
    "def precision_at_k(k, user_ratings):\n",
    "    n_rel = sum((relevant) for (_, _, relevant) in user_ratings[:k])\n",
    "    return n_rel / k\n",
    "\n",
    "# compute RR@k for one user\n",
    "def RR_at_k(k, user_ratings):\n",
    "    for i in range(k):\n",
    "        _, _, relevant = user_ratings[i]\n",
    "        if relevant == 1:\n",
    "            return 1.0 / (i+1)\n",
    "    return 0.0\n",
    "\n",
    "def total_relevant(user_ratings):\n",
    "    return sum((relevant) for (_, _, relevant) in user_ratings)\n",
    "\n",
    "def transfer_to_user_item_rating_relevant(pred_list):\n",
    "    # map the predictions to each user.\n",
    "    user_item_rating = defaultdict(list)\n",
    "    for pred in pred_list:\n",
    "        true_rating = user_item_test.loc[pred.uid, pred.iid] if pred.iid in list(user_item_test.columns) else 0\n",
    "        relevant = 1 if true_rating >= 4.0 else 0\n",
    "        user_item_rating[pred.uid].append((pred.iid, pred.est, relevant))\n",
    "    # {uid: [(iid, est, relevant)]}\n",
    "    return user_item_rating\n",
    "\n",
    "def compute_metrics(k, user_item_rating):\n",
    "    precisions = dict() # precision\n",
    "    ap = dict() # average precision\n",
    "    rr = dict() # reciprocal rank\n",
    "    for uid, user_ratings in user_item_rating.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        precisions[uid] = precision_at_k(k, user_ratings)\n",
    "        ap[uid] = (sum((precision_at_k(i, user_ratings)*user_ratings[i-1][2]) for i in range(1, k+1)) / total_relevant(user_ratings)) if total_relevant(user_ratings) != 0 else 0\n",
    "        rr[uid] = RR_at_k(k, user_ratings)\n",
    "\n",
    "    return sum(prec for prec in precisions.values()) / len(precisions), sum(prec for prec in ap.values()) / len(ap), sum(prec for prec in rr.values()) / len(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d10b188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Neighborhood based CF:\n",
      "Averaged P@5: 0.148\n",
      "MAP@5: 0.167\n",
      "MRR@5: 0.167\n",
      "Averaged P@10: 0.078\n",
      "MAP@10: 0.174\n",
      "MRR@10: 0.174\n",
      "Averaged P@20: 0.04\n",
      "MAP@20: 0.175\n",
      "MRR@20: 0.175\n",
      "\n",
      "Metrics for Latent Factor based CF:\n",
      "Averaged P@5: 0.024\n",
      "MAP@5: 0.073\n",
      "MRR@5: 0.073\n",
      "Averaged P@10: 0.05\n",
      "MAP@10: 0.116\n",
      "MRR@10: 0.116\n",
      "Averaged P@20: 0.038\n",
      "MAP@20: 0.137\n",
      "MRR@20: 0.137\n"
     ]
    }
   ],
   "source": [
    "user_item_rating_nb_relevant = transfer_to_user_item_rating_relevant(pred_nb_list_removed)\n",
    "user_item_rating_lf_relevant = transfer_to_user_item_rating_relevant(pred_lf_list_removed)\n",
    "\n",
    "k_set = [5, 10, 20]\n",
    "print(\"Metrics for Neighborhood based CF:\")\n",
    "for k in k_set:\n",
    "    p_at_k_nb, map_at_k_nb, mrr_at_k_nb = compute_metrics(k, user_item_rating_nb_relevant)\n",
    "    print(f\"Averaged P@{k}:\", round(p_at_k_nb, 3))\n",
    "    print(f\"MAP@{k}:\", round(map_at_k_nb, 3))\n",
    "    print(f\"MRR@{k}:\", round(mrr_at_k_nb, 3))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Metrics for Latent Factor based CF:\")\n",
    "for k in k_set:\n",
    "    p_at_k_lf, map_at_k_lf, mrr_at_k_lf = compute_metrics(k, user_item_rating_lf_relevant)\n",
    "    print(f\"Averaged P@{k}:\", round(p_at_k_lf, 3))\n",
    "    print(f\"MAP@{k}:\", round(map_at_k_lf, 3))\n",
    "    print(f\"MRR@{k}:\", round(mrr_at_k_lf, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Excercise 4\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 2, compute the systems’ hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "062588c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate for Neighborhood based CF:\n",
      "Hit Rate (top-5): 0.74\n",
      "Hit Rate (top-10): 0.781\n",
      "Hit Rate (top-20): 0.797\n",
      "\n",
      "Hit Rate for Latent Factor based CF:\n",
      "Hit Rate (top-5): 0.121\n",
      "Hit Rate (top-10): 0.502\n",
      "Hit Rate (top-20): 0.756\n"
     ]
    }
   ],
   "source": [
    "# compute HR@k for one user\n",
    "def HR_at_k(k, user_ratings):\n",
    "    for i in range(k):\n",
    "        _, _, relevant = user_ratings[i]\n",
    "        if relevant == 1:\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def compute_hit_rate(k, user_item_rating):\n",
    "    hr = dict() # hit rate\n",
    "    for uid, user_ratings in user_item_rating.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        hr[uid] = HR_at_k(k, user_ratings)\n",
    "\n",
    "    return sum(prec for prec in hr.values()) / len(hr) \n",
    "\n",
    "k_set = [5, 10, 20]\n",
    "print(\"Hit Rate for Neighborhood based CF:\")\n",
    "for k in k_set:\n",
    "    mhr_at_k_nb = compute_hit_rate(k, user_item_rating_nb_relevant)\n",
    "    print(f\"Hit Rate (top-{k}):\", round(mhr_at_k_nb, 3))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Hit Rate for Latent Factor based CF:\")\n",
    "for k in k_set:\n",
    "    mhr_at_k_lf = compute_hit_rate(k, user_item_rating_lf_relevant)\n",
    "    print(f\"Hit Rate (top-{k}):\", round(mhr_at_k_lf, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce644cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@5: (0.14794520547945267, 0.1674569722514925, 0.1674569722514925)\n",
      "P@10: (0.07808219178082199, 0.17352559921053023, 0.17352559921053023)\n",
      "P@20: (0.03983140147523708, 0.1746243276769131, 0.1746243276769131)\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# user_item_test = test_data.pivot('reviewerID', 'asin', 'overall')\n",
    "# user_item_test = user_item_test.fillna(0)\n",
    "\n",
    "# # compute RR@k for one user\n",
    "# def RR_at(relevants):\n",
    "#     for i in range(len(relevants)):\n",
    "#         if relevants[i] == 1.0:\n",
    "#             return 1.0 / (i+1)\n",
    "#     return 0.0\n",
    "\n",
    "# def precision_at(relevants):\n",
    "#     result = []\n",
    "#     sum_k = 0\n",
    "#     for i in range(len(relevants)):\n",
    "#         sum_k += relevants[i]\n",
    "#         if relevants[i] == 1.0:\n",
    "#             result.append(sum_k / (i+1))\n",
    "#     return result\n",
    "\n",
    "# def precision_at_k(k, top_k_list, user_item_rating):\n",
    "#     precisions = dict() # precision\n",
    "#     ap = dict() # average precision\n",
    "#     rr = dict() # reciprocal rank\n",
    "#     for uid, top_k_value in top_k_list.items():\n",
    "#         relevants = []\n",
    "#         for iid, _ in top_k_value:\n",
    "#             if iid in list(user_item_test.columns):\n",
    "#                 relevant = 1.0 if user_item_test.loc[uid, iid] >= 4.0 else 0 \n",
    "#                 relevants.append(relevant)\n",
    "#             else:\n",
    "#                 relevants.append(0)\n",
    "#         precisions[uid] = sum(relevants) / k\n",
    "#         ap[uid] = (sum(precision_at(relevants)) / total_relevant(user_item_rating[uid])) if total_relevant(user_item_rating[uid]) != 0 else 0 \n",
    "#         rr[uid] = RR_at(relevants)\n",
    "#     return sum(prec for prec in precisions.values()) / len(precisions), sum(prec for prec in rr.values()) / len(rr), sum(prec for prec in ap.values()) / len(ap)\n",
    "        \n",
    "# print(\"P@5:\", precision_at_k(5, top_5_nb, user_item_rating_nb_relevant))\n",
    "# print(\"P@10:\", precision_at_k(10, top_10_nb, user_item_rating_nb_relevant))\n",
    "# print(\"P@20:\", precision_at_k(20, top_20_nb, user_item_rating_nb_relevant))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
